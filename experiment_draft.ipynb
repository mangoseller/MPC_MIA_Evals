{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUHL7zZmSJTQ"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import crypten\n",
        "import crypten.nn as cnn\n",
        "import time\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5RpGIyKmouq"
      },
      "outputs": [],
      "source": [
        "def load_dataset(batch_size=128, num_workers=2):\n",
        "    temp_dataset = datasets.CIFAR10(root='./data', train=True, download=True,\n",
        "                                     transform=transforms.ToTensor())\n",
        "    temp_loader = DataLoader(temp_dataset, batch_size=batch_size, num_workers=num_workers)\n",
        "\n",
        "    channels_sum = t.zeros(3)\n",
        "    channels_squared_sum = t.zeros(3)\n",
        "    num_pixels = 0\n",
        "\n",
        "    for images, _ in temp_loader:\n",
        "        channels_sum += images.sum(dim=[0, 2, 3])\n",
        "        channels_squared_sum += (images ** 2).sum(dim=[0, 2, 3])\n",
        "        num_pixels += images.size(0) * images.size(2) * images.size(3)\n",
        "\n",
        "    mean = channels_sum / num_pixels\n",
        "    std = ((channels_squared_sum / num_pixels) - (mean ** 2)) ** 0.5\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean.tolist(), std.tolist())\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9AfmmWtoOyV"
      },
      "outputs": [],
      "source": [
        "# TODO: Generalise these, add PlainTextTanh models, ensure the architecture follows SigGuard/MPCDiff closely, Test end to end\n",
        "class PlainTextCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.activation1 = nn.Sigmoid()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.activation2 = nn.Sigmoid()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
        "        self.activation3 = nn.Sigmoid()\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            self.conv1,\n",
        "            self.activation1,\n",
        "            self.pool1,\n",
        "            self.conv2,\n",
        "            self.activation2,\n",
        "            self.pool2,\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.activation3,\n",
        "            self.fc2\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "class PlainTextMLP(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(3072, 512)\n",
        "        self.activation1 = nn.Sigmoid()\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.activation2 = nn.Sigmoid()\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.activation1,\n",
        "            self.fc2,\n",
        "            self.activation2,\n",
        "            self.fc3\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class PlainTextLeNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5, padding=2)\n",
        "        self.activation1 = nn.Sigmoid()\n",
        "        self.pool1 = nn.AvgPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.activation2 = nn.Sigmoid()\n",
        "        self.pool2 = nn.AvgPool2d(2, 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
        "        self.activation3 = nn.Sigmoid()\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.activation4 = nn.Sigmoid()\n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            self.conv1,\n",
        "            self.activation1,\n",
        "            self.pool1,\n",
        "            self.conv2,\n",
        "            self.activation2,\n",
        "            self.pool2,\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.activation3,\n",
        "            self.fc2,\n",
        "            self.activation4,\n",
        "            self.fc3\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "EnLKsJQ8qjqK"
      },
      "outputs": [],
      "source": [
        "def plaintext_train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc='Training', leave=False)\n",
        "    for inputs, targets in pbar:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        predictions = outputs.argmax(dim=1)\n",
        "        correct += (predictions == targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "\n",
        "        pbar.set_postfix({'loss': running_loss / (pbar.n + 1), 'acc': 100.0 * correct / total})\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100.0 * correct / total\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def plaintext_train_model(model, train_loader, num_epochs=10, lr=0.001, device='cuda'):\n",
        "    model = model.to(device)\n",
        "    optimizer = t.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = plaintext_train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {train_loss:.4f} | Acc: {train_acc:.2f}%')\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0vDQnx0quxV",
        "outputId": "9ac2eb31-797b-498a-fc99-1a2cfde4f130"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training PlainTextCNN...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                                \r"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     25\u001b[39m         t.save(trained_model.state_dict(), final_weights_path)\n\u001b[32m     26\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mFinal weights saved: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_weights_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mtrain_plaintext_models\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m40\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mtrain_plaintext_models\u001b[39m\u001b[34m(epochs)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models.items():\n\u001b[32m     15\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     trained_model, history = \u001b[43mplaintext_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m     final_weights_path = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m./weights/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_final.pt\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     25\u001b[39m     t.save(trained_model.state_dict(), final_weights_path)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36mplaintext_train_model\u001b[39m\u001b[34m(model, train_loader, num_epochs, lr, device)\u001b[39m\n\u001b[32m     35\u001b[39m history = {\n\u001b[32m     36\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m: [],\n\u001b[32m     37\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtrain_acc\u001b[39m\u001b[33m'\u001b[39m: []\n\u001b[32m     38\u001b[39m }\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     train_loss, train_acc = \u001b[43mplaintext_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m     history[\u001b[33m'\u001b[39m\u001b[33mtrain_loss\u001b[39m\u001b[33m'\u001b[39m].append(train_loss)\n\u001b[32m     44\u001b[39m     history[\u001b[33m'\u001b[39m\u001b[33mtrain_acc\u001b[39m\u001b[33m'\u001b[39m].append(train_acc)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mplaintext_train_epoch\u001b[39m\u001b[34m(model, train_loader, optimizer, criterion, device)\u001b[39m\n\u001b[32m      9\u001b[39m inputs, targets = inputs.to(device), targets.to(device)\n\u001b[32m     11\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m loss = criterion(outputs, targets)\n\u001b[32m     14\u001b[39m loss.backward()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MPC_DRAFT/xp_draft/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MPC_DRAFT/xp_draft/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 29\u001b[39m, in \u001b[36mPlainTextCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MPC_DRAFT/xp_draft/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MPC_DRAFT/xp_draft/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MPC_DRAFT/xp_draft/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MPC_DRAFT/xp_draft/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MPC_DRAFT/xp_draft/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MPC_DRAFT/xp_draft/lib/python3.12/site-packages/torch/nn/modules/pooling.py:226\u001b[39m, in \u001b[36mMaxPool2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[32m    225\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs the forward pass.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m226\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MPC_DRAFT/xp_draft/lib/python3.12/site-packages/torch/_jit_internal.py:627\u001b[39m, in \u001b[36mboolean_dispatch.<locals>.fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    625\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(*args, **kwargs)\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MPC_DRAFT/xp_draft/lib/python3.12/site-packages/torch/nn/functional.py:823\u001b[39m, in \u001b[36m_max_pool2d\u001b[39m\u001b[34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[39m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    822\u001b[39m     stride = torch.jit.annotate(\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "def train_plaintext_models(epochs=10):\n",
        "    device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
        "\n",
        "    train_loader, test_loader = load_dataset(batch_size=128, num_workers=2)\n",
        "\n",
        "    models = {\n",
        "        'PlainTextCNN': PlainTextCNN(num_classes=10),\n",
        "        'PlainTextMLP': PlainTextMLP(num_classes=10),\n",
        "        'PlainTextLeNet': PlainTextLeNet(num_classes=10)\n",
        "    }\n",
        "\n",
        "    os.makedirs('./weights', exist_ok=True)\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        print(f'\\nTraining {model_name}...')\n",
        "        trained_model, history = plaintext_train_model(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            num_epochs=epochs,\n",
        "            lr=1e-3,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        final_weights_path = f'./weights/{model_name}_final.pt'\n",
        "        t.save(trained_model.state_dict(), final_weights_path)\n",
        "        print(f'Final weights saved: {final_weights_path}')\n",
        "\n",
        "\n",
        "train_plaintext_models(40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jU9rUo2PqcoC"
      },
      "outputs": [],
      "source": [
        "def evaluate_accuracy_loss(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with t.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            predictions = outputs.argmax(dim=1)\n",
        "            correct += (predictions == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    avg_loss = running_loss / len(test_loader)\n",
        "    accuracy = 100.0 * correct / total\n",
        "\n",
        "    return avg_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMQrARNevlfE",
        "outputId": "10374f39-700c-4391-cbdc-eab6a11a8590"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './weights/PlainTextLeNet_final.pt'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 54\u001b[39m\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n\u001b[32m     53\u001b[39m train_loader, test_loader = load_dataset(batch_size=\u001b[32m128\u001b[39m, num_workers=\u001b[32m2\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m model, history = \u001b[43mload_and_continue_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPlainTextLeNet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./weights/PlainTextLeNet_final.pt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstart_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m150\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./weights/PlainTextCNN_LeNet_final.pt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\n\u001b[32m     63\u001b[39m \u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 40\u001b[39m, in \u001b[36mload_and_continue_training\u001b[39m\u001b[34m(model_class, weights_path, train_loader, num_epochs, lr, device, start_epoch, save_path)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_and_continue_training\u001b[39m(model_class, weights_path, train_loader, num_epochs=\u001b[32m10\u001b[39m,\n\u001b[32m     38\u001b[39m                                 lr=\u001b[32m0.001\u001b[39m, device=\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m, start_epoch=\u001b[32m0\u001b[39m, save_path=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     model = \u001b[43mload_model_from_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m     model, history = continue_training(\n\u001b[32m     42\u001b[39m         model=model,\n\u001b[32m     43\u001b[39m         train_loader=train_loader,\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m         save_path=save_path\n\u001b[32m     49\u001b[39m     )\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mload_model_from_weights\u001b[39m\u001b[34m(model_class, weights_path, num_classes, device)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_model_from_weights\u001b[39m(model_class, weights_path, num_classes=\u001b[32m10\u001b[39m, device=\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m      2\u001b[39m     model = model_class(num_classes=num_classes)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     model.load_state_dict(\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweights_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m      4\u001b[39m     model = model.to(device)\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLoaded weights from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mweights_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MPC_DRAFT/xp_draft/lib/python3.12/site-packages/torch/serialization.py:1484\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1481\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1482\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1484\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1485\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1486\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1487\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1488\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1489\u001b[39m         orig_position = opened_file.tell()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MPC_DRAFT/xp_draft/lib/python3.12/site-packages/torch/serialization.py:759\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    757\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer: FileLike, mode: \u001b[38;5;28mstr\u001b[39m) -> _opener[IO[\u001b[38;5;28mbytes\u001b[39m]]:\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m759\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    760\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    761\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MPC_DRAFT/xp_draft/lib/python3.12/site-packages/torch/serialization.py:740\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    739\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike[\u001b[38;5;28mstr\u001b[39m]], mode: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m740\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: './weights/PlainTextLeNet_final.pt'"
          ]
        }
      ],
      "source": [
        "def load_model_from_weights(model_class, weights_path, num_classes=10, device='cuda'):\n",
        "    model = model_class(num_classes=num_classes)\n",
        "    model.load_state_dict(t.load(weights_path, map_location=device, weights_only=True))\n",
        "    model = model.to(device)\n",
        "    print(f'Loaded weights from: {weights_path}')\n",
        "    return model\n",
        "\n",
        "\n",
        "def continue_training(model, train_loader, num_epochs=10, lr=0.001, device='cuda',\n",
        "                      start_epoch=0, save_path=None):\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = t.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = plaintext_train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "\n",
        "        display_epoch = start_epoch + epoch + 1\n",
        "        print(f'Epoch {display_epoch} - Loss: {train_loss:.4f} | Acc: {train_acc:.2f}%')\n",
        "\n",
        "    if save_path is not None:\n",
        "        t.save(model.state_dict(), save_path)\n",
        "        print(f'Weights saved: {save_path}')\n",
        "\n",
        "    return model, history\n",
        "\n",
        "\n",
        "def load_and_continue_training(model_class, weights_path, train_loader, num_epochs=10,\n",
        "                                lr=0.001, device='cuda', start_epoch=0, save_path=None):\n",
        "\n",
        "    model = load_model_from_weights(model_class, weights_path, device=device)\n",
        "    model, history = continue_training(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        num_epochs=num_epochs,\n",
        "        lr=lr,\n",
        "        device=device,\n",
        "        start_epoch=start_epoch,\n",
        "        save_path=save_path\n",
        "    )\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CrypTen models require manual flattening\n",
        "\n",
        "class MpcFlatten(cnn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.flatten(start_dim=1)\n",
        "\n",
        "class MpcCNN(cnn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = cnn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.activation1 = cnn.Sigmoid() \n",
        "        # Note that MaxPool2d is significantly more expensive in MPC\n",
        "        self.pool1 = cnn.MaxPool2d(2, 2)\n",
        "        self.conv2 = cnn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.activation2 = cnn.Sigmoid()\n",
        "        self.pool2 = cnn.MaxPool2d(2, 2)\n",
        "        \n",
        "        self.flatten = MpcFlatten() # replace nn.Flatten\n",
        "        \n",
        "        self.fc1 = cnn.Linear(64 * 8 * 8, 512)\n",
        "        self.activation3 = cnn.Sigmoid()\n",
        "        self.fc2 = cnn.Linear(512, num_classes)\n",
        "\n",
        "        self.network = cnn.Sequential(\n",
        "            self.conv1,\n",
        "            self.activation1,\n",
        "            self.pool1,\n",
        "            self.conv2,\n",
        "            self.activation2,\n",
        "            self.pool2,\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.activation3,\n",
        "            self.fc2\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "class MpcMLP(cnn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.flatten = MpcFlatten() \n",
        "        self.fc1 = cnn.Linear(3072, 512)\n",
        "        self.activation1 = cnn.Sigmoid()\n",
        "        self.fc2 = cnn.Linear(512, 256)\n",
        "        self.activation2 = cnn.Sigmoid()\n",
        "        self.fc3 = cnn.Linear(256, num_classes)\n",
        "\n",
        "        self.network = cnn.Sequential(\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.activation1,\n",
        "            self.fc2,\n",
        "            self.activation2,\n",
        "            self.fc3\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class MpcLeNet(cnn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = cnn.Conv2d(3, 6, kernel_size=5, padding=2)\n",
        "        self.activation1 = cnn.Sigmoid()\n",
        "        self.pool1 = cnn.AvgPool2d(2, 2) # AvgPool is efficient in MPC\n",
        "        \n",
        "        self.conv2 = cnn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.activation2 = cnn.Sigmoid()\n",
        "        self.pool2 = cnn.AvgPool2d(2, 2)\n",
        "        \n",
        "        self.flatten = MpcFlatten() \n",
        "        self.fc1 = cnn.Linear(16 * 6 * 6, 120)\n",
        "        self.activation3 = cnn.Sigmoid()\n",
        "        self.fc2 = cnn.Linear(120, 84)\n",
        "        self.activation4 = cnn.Sigmoid()\n",
        "        self.fc3 = cnn.Linear(84, num_classes)\n",
        "\n",
        "        self.network = cnn.Sequential(\n",
        "            self.conv1,\n",
        "            self.activation1,\n",
        "            self.pool1,\n",
        "            self.conv2,\n",
        "            self.activation2,\n",
        "            self.pool2,\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.activation3,\n",
        "            self.fc2,\n",
        "            self.activation4,\n",
        "            self.fc3\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# TODO: Just generalize the construction of these models so we can change the activation functions easily\n",
        "class MpcCNNTanh(cnn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = cnn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.activation1 = cnn.Tanh()  \n",
        "        self.pool1 = cnn.MaxPool2d(2, 2)\n",
        "        \n",
        "        self.conv2 = cnn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.activation2 = cnn.Tanh()  \n",
        "        self.pool2 = cnn.MaxPool2d(2, 2)\n",
        "        \n",
        "        self.flatten = MpcFlatten() \n",
        "        \n",
        "        self.fc1 = cnn.Linear(64 * 8 * 8, 512)\n",
        "        self.activation3 = cnn.Tanh()  \n",
        "        self.fc2 = cnn.Linear(512, num_classes)\n",
        "\n",
        "        self.network = cnn.Sequential(\n",
        "            self.conv1,\n",
        "            self.activation1,\n",
        "            self.pool1,\n",
        "            self.conv2,\n",
        "            self.activation2,\n",
        "            self.pool2,\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.activation3,\n",
        "            self.fc2\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "class MpcMLPTanh(cnn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.flatten = MpcFlatten()\n",
        "        self.fc1 = cnn.Linear(3072, 512)\n",
        "        self.activation1 = cnn.Tanh()  \n",
        "        self.fc2 = cnn.Linear(512, 256)\n",
        "        self.activation2 = cnn.Tanh()  \n",
        "        self.fc3 = cnn.Linear(256, num_classes)\n",
        "\n",
        "        self.network = cnn.Sequential(\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.activation1,\n",
        "            self.fc2,\n",
        "            self.activation2,\n",
        "            self.fc3\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "class MpcLeNetTanh(cnn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.conv1 = cnn.Conv2d(3, 6, kernel_size=5, padding=2)\n",
        "        self.activation1 = cnn.Tanh()\n",
        "        self.pool1 = cnn.AvgPool2d(2, 2)\n",
        "        \n",
        "        self.conv2 = cnn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.activation2 = cnn.Tanh()  \n",
        "        self.pool2 = cnn.AvgPool2d(2, 2)\n",
        "        \n",
        "        self.flatten = MpcFlatten()\n",
        "        \n",
        "        self.fc1 = cnn.Linear(16 * 6 * 6, 120)\n",
        "        self.activation3 = cnn.Tanh()  \n",
        "        self.fc2 = cnn.Linear(120, 84)\n",
        "        self.activation4 = cnn.Tanh()  \n",
        "        self.fc3 = cnn.Linear(84, num_classes)\n",
        "\n",
        "        self.network = cnn.Sequential(\n",
        "            self.conv1,\n",
        "            self.activation1,\n",
        "            self.pool1,\n",
        "            self.conv2,\n",
        "            self.activation2,\n",
        "            self.pool2,\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.activation3,\n",
        "            self.fc2,\n",
        "            self.activation4,\n",
        "            self.fc3\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mpc_train_epoch(model, train_loader, optimizer, criterion, device, num_classes=10):\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "        x_enc = crypten.cryptensor(inputs) # Encrypt inputs      \n",
        "        y_one_hot = F.one_hot(targets, num_classes=num_classes).float() # Prepare Targets (One-Hot Encoding is required for MPC Loss) (TODO: Look into this)\n",
        "        y_enc = crypten.cryptensor(y_one_hot)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output_enc = model(x_enc) # Encrypted Forward Pass\n",
        "        loss_enc = criterion(output_enc, y_enc)\n",
        "        loss_enc.backward() # Encrypted Backward Pass \n",
        "        optimizer.step()   \n",
        "\n",
        "        loss_val = loss_enc.get_plain_text().item() # Decrypt for metrics calculation \n",
        "        running_loss += loss_val\n",
        "        \n",
        "        # Decrypt outputs to calculate accuracy \n",
        "        output_plain = output_enc.get_plain_text()\n",
        "        predictions = output_plain.argmax(dim=1)\n",
        "        correct += (predictions == targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100.0 * correct / total\n",
        "    \n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def mpc_train_model(model, train_loader, num_epochs=10, lr=0.001, device='cpu'):\n",
        "\n",
        "    if not crypten.is_initialized():\n",
        "        crypten.init()\n",
        "    \n",
        "    model.encrypt() # Encrypt the model (Weights become CrypTensors)\n",
        "    model.train()\n",
        "\n",
        "    optimizer = crypten.optim.Adam(model.parameters(), lr=lr) # Use CrypTen implementation of Adam\n",
        "    criterion = cnn.CrossEntropyLoss() # Crypten implementation of CrossEntropyLoss\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': []\n",
        "    }\n",
        "\n",
        "    print(f\"Starting MPC Training for {num_epochs} epochs...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = mpc_train_epoch(\n",
        "            model, \n",
        "            train_loader, \n",
        "            optimizer, \n",
        "            criterion, \n",
        "            device\n",
        "        )\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        \n",
        "        elapsed = time.time() - start_time\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {train_loss:.4f} | Acc: {train_acc:.2f}% | Time: {elapsed:.0f}s')\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def train_mpc_models(epochs=10):\n",
        "\n",
        "    train_loader, test_loader = load_dataset(batch_size=32, num_workers=2) # Reduced batch size for MPC memory safety\n",
        "\n",
        "    models = {\n",
        "        'MpcCNN': MpcCNN(num_classes=10),\n",
        "        'MpcMLP': MpcMLP(num_classes=10),\n",
        "        'MpcLeNet': MpcLeNet(num_classes=10)\n",
        "    }\n",
        "\n",
        "    os.makedirs('./weights_mpc', exist_ok=True)\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        print(f'\\nTraining {model_name} in MPC...')\n",
        "        \n",
        "        trained_model, history = mpc_train_model(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            num_epochs=epochs,\n",
        "            lr=1e-3\n",
        "        )\n",
        "        \n",
        "        # Save encrypted model state\n",
        "        final_weights_path = f'./weights_mpc/{model_name}_encrypted.pt'\n",
        "        crypten.save(trained_model.state_dict(), final_weights_path)\n",
        "        print(f'Encrypted weights saved: {final_weights_path}')\n",
        "\n",
        "train_mpc_models(epochs=1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_shadow_models(num_shadows, model_class, full_dataset, num_epochs=10, device='cuda'):\n",
        "    \"\"\"\n",
        "    Trains a set of shadow models on random subsets of the data. # TODO: Fix a portion of CIFAR10/Whatever dataset to train shadow models on? \n",
        "    Returns:\n",
        "        shadow_models (list): List of trained shadow models.\n",
        "        shadow_data (list): List of tuples (train_indices, test_indices) used for each model.\n",
        "    \"\"\"\n",
        "    shadow_models = []\n",
        "    shadow_data_indices = []\n",
        "    dataset_size = len(full_dataset)\n",
        "    split_size = dataset_size // 2  # 50% in, 50% out (Check Shokri Figures)\n",
        "    \n",
        "    print(f\"Training {num_shadows} shadow models...\")\n",
        "    \n",
        "    for i in range(num_shadows):\n",
        "        indices = np.random.permutation(dataset_size)  # Create random split for this shadow model\n",
        "        train_indices = indices[:split_size]\n",
        "        test_indices = indices[split_size:]        \n",
        "        train_subset = Subset(full_dataset, train_indices)\n",
        "\n",
        "        train_loader = DataLoader(train_subset, batch_size=128, shuffle=True, num_workers=0)\n",
        "        shadow_model = model_class(num_classes=10).to(device) # Initialize and a train shadow model\n",
        "        \n",
        "        print(f\"Shadow Model {i+1}/{num_shadows}...\")\n",
        "        plaintext_train_model(\n",
        "            shadow_model, \n",
        "            train_loader, \n",
        "            num_epochs=num_epochs, \n",
        "            lr=1e-3, \n",
        "            device=device\n",
        "        )\n",
        "        shadow_models.append(shadow_model)\n",
        "        shadow_data_indices.append((train_indices, test_indices))\n",
        "        \n",
        "    return shadow_models, shadow_data_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttackNet(nn.Module):\n",
        "    def __init__(self, input_dim=10):\n",
        "        super().__init__()\n",
        "        # Input is the target model's logit vector (size 10 for CIFAR-10) \n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.activation1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.activation2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(32, 1) # 0 Non-Member, 1 Member\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation1(self.fc1(x))\n",
        "        x = self.activation2(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "def prepare_attack_dataset(shadow_models, shadow_indices, full_dataset, device='cuda'):\n",
        " #   Generate (logit, membership_label) pairs from shadow models.\n",
        "    X_attack = []\n",
        "    y_attack = []\n",
        "    \n",
        "    full_loader = DataLoader(full_dataset, batch_size=128, shuffle=False, num_workers=0)\n",
        "    \n",
        "    with t.no_grad():\n",
        "        for i, model in enumerate(shadow_models):\n",
        "            model.eval()\n",
        "            train_idx, test_idx = shadow_indices[i]\n",
        "            train_set = set(train_idx)\n",
        "            \n",
        "            # Get predictions for the whole dataset\n",
        "            all_preds = []\n",
        "            for inputs, _ in full_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                outputs = model(inputs)\n",
        "                preds = F.softmax(outputs, dim=1)# Convert logits to probabilities\n",
        "                all_preds.append(preds.cpu())\n",
        "\n",
        "            all_preds = t.cat(all_preds)\n",
        "            \n",
        "            # Label '1' for members (in train_set), '0' for non-members\n",
        "\n",
        "            for idx in range(len(full_dataset)):\n",
        "                pred_vector = all_preds[idx]\n",
        "                label = 1.0 if idx in train_set else 0.0\n",
        "                \n",
        "                X_attack.append(pred_vector)\n",
        "                y_attack.append(label)\n",
        "                \n",
        "    X_attack = t.stack(X_attack)\n",
        "    y_attack = t.tensor(y_attack).unsqueeze(1) # Shape [N, 1]\n",
        "    \n",
        "    return X_attack, y_attack\n",
        "\n",
        "def train_attack_model(X_attack, y_attack, epochs=20, device='cuda'):\n",
        "\n",
        "    attack_model = AttackNet().to(device)\n",
        "    optimizer = t.optim.Adam(attack_model.parameters(), lr=0.001)\n",
        "    criterion = nn.BCELoss() # Binary Cross Entropy Loss\n",
        "    \n",
        "    dataset = t.utils.data.TensorDataset(X_attack, y_attack)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "    \n",
        "    print(\"Training Attack Model...\")\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = attack_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "    return attack_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_mia_attack(target_model, attack_model, train_loader, test_loader, device, is_mpc=False):\n",
        "    \"\"\"\n",
        "    Runs the trained attack model on the target model's members (train_loader) \n",
        "    and non-members (test_loader) to calculate attack accuracy.\n",
        "    TODO: Construct a mixed dataset to test on\n",
        "    \"\"\"\n",
        "\n",
        "    target_model.eval()\n",
        "    attack_model.eval()\n",
        "    \n",
        "    # Helper to get predictions from the target\n",
        "\n",
        "    def get_target_preds(loader, is_member):\n",
        "        preds = []\n",
        "        labels = []\n",
        "        \n",
        "        for inputs, _ in loader:\n",
        "\n",
        "            if is_mpc:\n",
        "\n",
        "                # MPC Path: Encrypt -> Forward -> Decrypt\n",
        "                x_enc = crypten.cryptensor(inputs)\n",
        "                output_enc = target_model(x_enc)\n",
        "                output_plain = output_enc.get_plain_text()\n",
        "                \n",
        "                # Apply softmax on plaintext for the attack features\n",
        "                # (Attacker sees probabilities)\n",
        "                batch_preds = F.softmax(output_plain, dim = 1)\n",
        "\n",
        "            else:\n",
        "                # Plaintext Path\n",
        "                inputs = inputs.to(device)\n",
        "                with t.no_grad():\n",
        "                    outputs = target_model(inputs)\n",
        "                    batch_preds = softmax(outputs)\n",
        "            \n",
        "            preds.append(batch_preds.cpu())\n",
        "            # 1.0 for Members, 0.0 for Non-Members\n",
        "            labels.extend([1.0 if is_member else 0.0] * inputs.size(0))\n",
        "        return t.cat(preds), t.tensor(labels).unsqueeze(1)\n",
        "\n",
        "    print(f\"Collecting predictions from {'MPC' if is_mpc else 'Plaintext'} Target...\")\n",
        "    \n",
        "    # Get predictions on Member data (Train set)\n",
        "    member_preds, member_labels = get_target_preds(train_loader, is_member=True)\n",
        "    \n",
        "    # Get predictions on Non-Member data (Test set)\n",
        "    non_member_preds, non_member_labels = get_target_preds(test_loader, is_member=False)\n",
        "    \n",
        "    all_preds = t.cat([member_preds, non_member_preds])\n",
        "    all_labels = t.cat([member_labels, non_member_labels])\n",
        "    \n",
        "    with t.no_grad():\n",
        "        attack_probs = attack_model(all_preds.to(device))\n",
        "        attack_preds = (attack_probs > 0.5).float().cpu()\n",
        "        \n",
        "    correct = (attack_preds == all_labels).sum().item()\n",
        "    total = all_labels.size(0)\n",
        "    accuracy = 100.0 * correct / total\n",
        "    \n",
        "    tp = ((attack_preds == 1) & (all_labels == 1)).sum().item()\n",
        "    fp = ((attack_preds == 1) & (all_labels == 0)).sum().item()\n",
        "    fn = ((attack_preds == 0) & (all_labels == 1)).sum().item()\n",
        "    \n",
        "    precision = 100.0 * tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = 100.0 * tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    \n",
        "    print(f\"MIA Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"MIA Precision (Member): {precision:.2f}%\")\n",
        "    print(f\"MIA Recall (Member): {recall:.2f}%\")\n",
        "    \n",
        "    return accuracy, precision, recall"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "crypten_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
