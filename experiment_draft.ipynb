{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "zUHL7zZmSJTQ"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import os\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import crypten\n",
        "import crypten.nn as cnn\n",
        "import time\n",
        "from copy import deepcopy\n",
        "from functools import partial\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "k9AfmmWtoOyV"
      },
      "outputs": [],
      "source": [
        "\"\"\"Plaintext models with parameterized activation functions.\n",
        "Pass activation_fn for hidden layers, pass output_fn to \n",
        "apply an activation after the final layer\"\"\"\n",
        "\n",
        "class PlainTextCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10, activation_fn=nn.Sigmoid):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.activation1 = activation_fn()\n",
        "        self.pool1 = nn.AvgPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.activation2 = activation_fn()\n",
        "        self.pool2 = nn.AvgPool2d(2, 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
        "        self.activation3 = activation_fn()\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "        layers = [\n",
        "            self.conv1,\n",
        "            self.activation1,\n",
        "            self.pool1,\n",
        "            self.conv2,\n",
        "            self.activation2,\n",
        "            self.pool2,\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.activation3,\n",
        "            self.fc2\n",
        "        ]\n",
        "        \n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "class PlainTextMLP(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=10, activation_fn=nn.Sigmoid):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(3072, 512)\n",
        "        self.activation1 = activation_fn()\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.activation2 = activation_fn()\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "        layers = [\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.activation1,\n",
        "            self.fc2,\n",
        "            self.activation2,\n",
        "            self.fc3\n",
        "        ]\n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "class PlainTextLeNet(nn.Module):\n",
        "    def __init__(self, num_classes=10, activation_fn=nn.Sigmoid):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(6)\n",
        "        self.activation1 = activation_fn()\n",
        "        self.pool1 = nn.AvgPool2d(2, 2)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.activation2 = activation_fn()\n",
        "        self.pool2 = nn.AvgPool2d(2, 2)\n",
        "        \n",
        "        self.flatten = nn.Flatten()\n",
        "        \n",
        "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
        "        self.bn3 = nn.BatchNorm1d(120)\n",
        "        self.activation3 = activation_fn()\n",
        "        \n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.bn4 = nn.BatchNorm1d(84)\n",
        "        self.activation4 = activation_fn()\n",
        "        \n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "        layers = [\n",
        "            self.conv1,\n",
        "            self.bn1,\n",
        "            self.activation1,\n",
        "            self.pool1,\n",
        "            self.conv2,\n",
        "            self.bn2,\n",
        "            self.activation2,\n",
        "            self.pool2,\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.bn3,\n",
        "            self.activation3,\n",
        "            self.fc2,\n",
        "            self.bn4,\n",
        "            self.activation4,\n",
        "            self.fc3\n",
        "        ]\n",
        "        \n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"CrypTen models with parameterized activation functions.\n",
        "   CrypTen requires manual flattening and uses cnn.* modules.\n",
        "   Pass output_fn to apply an activation after the final layer\"\"\"\n",
        "   \n",
        "# TODO: Investigate how these activation functions are being approximated exactly, and replace them where required\n",
        "\n",
        "class MpcFlatten(cnn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.flatten(start_dim=1)\n",
        "\n",
        "\n",
        "class MpcTanh(cnn.Module):\n",
        "    # Wrapper for Tanh activation in CrypTen.\n",
        "    def forward(self, x):\n",
        "        return x.tanh()\n",
        "\n",
        "class MpcCNN(cnn.Module):\n",
        "\n",
        "    def __init__(self, num_classes=10, activation_fn=cnn.Sigmoid):\n",
        "        super().__init__()\n",
        "        self.conv1 = cnn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.activation1 = activation_fn()\n",
        "        self.pool1 = cnn.AvgPool2d(2, 2)\n",
        "        self.conv2 = cnn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.activation2 = activation_fn()\n",
        "        self.pool2 = cnn.AvgPool2d(2, 2)\n",
        "        \n",
        "        self.flatten = MpcFlatten()\n",
        "        \n",
        "        self.fc1 = cnn.Linear(64 * 8 * 8, 512)\n",
        "        self.activation3 = activation_fn()\n",
        "        self.fc2 = cnn.Linear(512, num_classes)\n",
        "\n",
        "        layers = [\n",
        "            self.conv1,\n",
        "            self.activation1,\n",
        "            self.pool1,\n",
        "            self.conv2,\n",
        "            self.activation2,\n",
        "            self.pool2,\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.activation3,\n",
        "            self.fc2\n",
        "        ]\n",
        "\n",
        "        self.network = cnn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "class MpcMLP(cnn.Module):\n",
        "    def __init__(self, num_classes=10, activation_fn=cnn.Sigmoid):\n",
        "        super().__init__()\n",
        "        self.flatten = MpcFlatten()\n",
        "        self.fc1 = cnn.Linear(3072, 512)\n",
        "        self.activation1 = activation_fn()\n",
        "        self.fc2 = cnn.Linear(512, 256)\n",
        "        self.activation2 = activation_fn()\n",
        "        self.fc3 = cnn.Linear(256, num_classes)\n",
        "\n",
        "        layers = [\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.activation1,\n",
        "            self.fc2,\n",
        "            self.activation2,\n",
        "            self.fc3\n",
        "        ]\n",
        "\n",
        "        self.network = cnn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "class MpcLeNet(cnn.Module):\n",
        "    def __init__(self, num_classes=10, activation_fn=cnn.Sigmoid):\n",
        "        super().__init__()\n",
        "        self.conv1 = cnn.Conv2d(3, 6, kernel_size=5, padding=2)\n",
        "        self.bn1 = cnn.BatchNorm2d(6)\n",
        "        self.activation1 = activation_fn()\n",
        "        self.pool1 = cnn.AvgPool2d(2, 2) \n",
        "        \n",
        "        self.conv2 = cnn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.bn2 = cnn.BatchNorm2d(16)\n",
        "        self.activation2 = activation_fn()\n",
        "        self.pool2 = cnn.AvgPool2d(2, 2)\n",
        "        \n",
        "        self.flatten = MpcFlatten()\n",
        "        \n",
        "        self.fc1 = cnn.Linear(16 * 6 * 6, 120)\n",
        "        self.bn3 = cnn.BatchNorm1d(120)\n",
        "        self.activation3 = activation_fn()\n",
        "        \n",
        "        self.fc2 = cnn.Linear(120, 84)\n",
        "        self.bn4 = cnn.BatchNorm1d(84)\n",
        "        self.activation4 = activation_fn()\n",
        "        \n",
        "        self.fc3 = cnn.Linear(84, num_classes)\n",
        "\n",
        "        layers = [\n",
        "            self.conv1,\n",
        "            self.bn1,\n",
        "            self.activation1,\n",
        "            self.pool1,\n",
        "            self.conv2,\n",
        "            self.bn2,\n",
        "            self.activation2,\n",
        "            self.pool2,\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.bn3,\n",
        "            self.activation3,\n",
        "            self.fc2,\n",
        "            self.bn4,\n",
        "            self.activation4,\n",
        "            self.fc3\n",
        "        ]\n",
        "        \n",
        "        self.network = cnn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "MPC_MODELS = {\n",
        "    'MpcCNN_Sigmoid': partial(MpcCNN, activation_fn=cnn.Sigmoid),\n",
        "    'MpcCNN_Tanh': partial(MpcCNN, activation_fn=MpcTanh),\n",
        "    'MpcCNN_ReLU': partial(MpcCNN, activation_fn=cnn.ReLU),\n",
        "    'MpcMLP_Sigmoid': partial(MpcMLP, activation_fn=cnn.Sigmoid),\n",
        "    'MpcMLP_Tanh': partial(MpcMLP, activation_fn=MpcTanh),\n",
        "    'MpcMLP_ReLU': partial(MpcMLP, activation_fn=cnn.ReLU),\n",
        "    'MpcLeNet_Sigmoid': partial(MpcLeNet, activation_fn=cnn.Sigmoid),\n",
        "    'MpcLeNet_Tanh': partial(MpcLeNet, activation_fn=MpcTanh),\n",
        "    'MpcLeNet_ReLU': partial(MpcLeNet, activation_fn=cnn.ReLU),\n",
        "}\n",
        "\n",
        "PLAINTEXT_MODELS = {\n",
        "    'PlainTextCNN_Sigmoid': partial(PlainTextCNN, activation_fn=nn.Sigmoid),\n",
        "    'PlainTextCNN_Tanh': partial(PlainTextCNN, activation_fn=nn.Tanh),\n",
        "    'PlainTextCNN_ReLU': partial(PlainTextCNN, activation_fn=nn.ReLU),\n",
        "    'PlainTextMLP_Sigmoid': partial(PlainTextMLP, activation_fn=nn.Sigmoid),\n",
        "    'PlainTextMLP_Tanh': partial(PlainTextMLP, activation_fn=nn.Tanh),\n",
        "    'PlainTextMLP_ReLU': partial(PlainTextMLP, activation_fn=nn.ReLU),\n",
        "    'PlainTextLeNet_Sigmoid': partial(PlainTextLeNet, activation_fn=nn.Sigmoid),\n",
        "    'PlainTextLeNet_Tanh': partial(PlainTextLeNet, activation_fn=nn.Tanh),\n",
        "    'PlainTextLeNet_ReLU': partial(PlainTextLeNet, activation_fn=nn.ReLU),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "EnLKsJQ8qjqK"
      },
      "outputs": [],
      "source": [
        "def plaintext_train_model(model, train_loader, num_epochs=10, lr=0.001, device='cuda', save_path=None):\n",
        "    model = model.to(device)\n",
        "    optimizer = t.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': []\n",
        "    }\n",
        "\n",
        "    print(f\"Starting Plaintext Training for {num_epochs} epochs...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            predictions = outputs.argmax(dim=1)\n",
        "            correct += (predictions == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        accuracy = 100.0 * correct / total\n",
        "        \n",
        "        history['train_loss'].append(avg_loss)\n",
        "        history['train_acc'].append(accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} complete: Loss: {avg_loss:.4f} | Acc: {accuracy:.2f}%\")\n",
        "\n",
        "    if save_path:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        t.save(model.state_dict(), save_path)\n",
        "        print(f\"Model saved to: {save_path}\")\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mpc_train_epoch(model, train_loader, optimizer, criterion, device, num_classes=10):\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "   \n",
        "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "\n",
        "        x_enc = crypten.cryptensor(inputs)\n",
        "        y_one_hot = F.one_hot(targets, num_classes=num_classes).float() # Crypten requires one-hot labels\n",
        "        y_enc = crypten.cryptensor(y_one_hot)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output_enc = model(x_enc)\n",
        "        loss_enc = criterion(output_enc, y_enc)\n",
        "        loss_enc.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_val = loss_enc.get_plain_text().item()\n",
        "        running_loss += loss_val\n",
        "        \n",
        "        # Decrypt predictions for accuracy\n",
        "        output_plain = output_enc.get_plain_text()\n",
        "        predictions = output_plain.argmax(dim=1)\n",
        "        correct += (predictions == targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(f\"Batch {batch_idx}/{len(train_loader)} | Loss: {loss_val:.4f}\")\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100.0 * correct / total\n",
        "    \n",
        "    return avg_loss, accuracy\n",
        "    \n",
        "def mpc_train_model(model, train_loader, num_epochs=10, lr=0.001, device='cpu', \n",
        "                    model_name='MpcModel', checkpoint_dir='./weights_mpc'):\n",
        "\n",
        "    if not crypten.is_initialized():\n",
        "        crypten.init()\n",
        "    \n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    \n",
        "    model.encrypt()\n",
        "    model.train()\n",
        "\n",
        "    optimizer = crypten.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    criterion = cnn.CrossEntropyLoss()\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': []\n",
        "    }\n",
        "\n",
        "    # Calculate checkpoint epochs (every 1/5th of training)\n",
        "    checkpoint_interval = max(1, num_epochs // 5)\n",
        "    checkpoint_epochs = set(range(checkpoint_interval, num_epochs + 1, checkpoint_interval))\n",
        "    # Always include final epoch\n",
        "    checkpoint_epochs.add(num_epochs)\n",
        "\n",
        "    print(f\"Checkpoints stored at epochs: {sorted(checkpoint_epochs)}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    print(f\"Starting MPC Training for {num_epochs} epochs...\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = mpc_train_epoch(\n",
        "            model, \n",
        "            train_loader, \n",
        "            optimizer, \n",
        "            criterion, \n",
        "            device\n",
        "        )\n",
        "        \n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        \n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} complete: Loss: {train_loss:.4f} | Acc: {train_acc:.2f}%\")\n",
        "        \n",
        "        elapsed = time.time() - start_time\n",
        "        current_epoch = epoch + 1\n",
        "\n",
        "        if current_epoch in checkpoint_epochs:\n",
        "            checkpoint_path = f'{checkpoint_dir}/{model_name}_epoch{current_epoch}.pt'\n",
        "            crypten.save(model.state_dict(), checkpoint_path)\n",
        "            print(f'Checkpoint saved at epoch {current_epoch}, at: {checkpoint_path}')\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def partition_dataset_for_mia(full_dataset, target_train_size, shadow_pool_ratio=0.5, seed=42):\n",
        "    \n",
        "# Partition dataset into disjoint pools for target model and shadow models\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    dataset_size = len(full_dataset)\n",
        "    all_indices = np.random.permutation(dataset_size)\n",
        "    \n",
        "    # Allocate target training set\n",
        "    target_train_indices = all_indices[:target_train_size]\n",
        "    remaining_indices = all_indices[target_train_size:]\n",
        "    \n",
        "    # Split remaining data between target test set and shadow pool\n",
        "    shadow_pool_size = int(len(remaining_indices) * shadow_pool_ratio)\n",
        "    shadow_pool_indices = remaining_indices[:shadow_pool_size]\n",
        "\n",
        "   # Slice the test indices to match target_train_size\n",
        "    target_test_indices = remaining_indices[shadow_pool_size : shadow_pool_size + target_train_size]\n",
        "    \n",
        "    print(f\"Target model training set size: {len(target_train_indices)}\")\n",
        "    print(f\"Target model testing set size:  {len(target_test_indices)}\")\n",
        "    print(f\"Shadow model training pool size:  {len(shadow_pool_indices)}\")\n",
        "    \n",
        "    return target_train_indices, target_test_indices, shadow_pool_indices\n",
        "\n",
        "\n",
        "def train_shadow_models(num_shadows, model_class, full_dataset, shadow_pool_indices, \n",
        "                        model_name, base_dir, num_epochs=10, device='cuda'):\n",
        " #   Trains shadow models on the shadow pool, whichi is disjoint from the target model's training data\n",
        "    \n",
        "    shadow_models = []\n",
        "    shadow_data_indices = []\n",
        "    shadow_pool_size = len(shadow_pool_indices)\n",
        "    split_size = shadow_pool_size // 2  # 50% in, 50% out \n",
        "    \n",
        "    shadow_save_dir = os.path.join(base_dir, model_name)\n",
        "    os.makedirs(shadow_save_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Training {num_shadows} shadow models on pool of {shadow_pool_size} samples...\")\n",
        "\n",
        "    \n",
        "    for i in range(num_shadows):\n",
        "        # Random permutation within the shadow pool (shadow datasets may overlap)\n",
        "        perm = np.random.permutation(shadow_pool_size)\n",
        "        train_local_indices = perm[:split_size]\n",
        "        test_local_indices = perm[split_size:]\n",
        "        \n",
        "        # Map local indices back to full dataset indices\n",
        "        train_indices = shadow_pool_indices[train_local_indices]\n",
        "        test_indices = shadow_pool_indices[test_local_indices]\n",
        "        \n",
        "        train_subset = Subset(full_dataset, train_indices)\n",
        "        train_loader = DataLoader(train_subset, batch_size=128, shuffle=True, num_workers=0)\n",
        "        \n",
        "        shadow_model = model_class(num_classes=10).to(device)\n",
        "        \n",
        "    # Save as shadow_0.pt, shadow_1.pt, etc.\n",
        "        save_path = os.path.join(shadow_save_dir, f\"shadow_{i}.pt\")\n",
        "        \n",
        "        print(f\"Shadow {i+1}/{num_shadows}...\", end=\" \")\n",
        "        plaintext_train_model(\n",
        "            shadow_model, \n",
        "            train_loader, \n",
        "            num_epochs=num_epochs, \n",
        "            lr=1e-3, \n",
        "            device=device,\n",
        "            save_path=save_path\n",
        "        )\n",
        "\n",
        "        shadow_models.append(shadow_model)\n",
        "        shadow_data_indices.append((train_indices, test_indices))\n",
        "        \n",
        "    return shadow_models, shadow_data_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttackNet(nn.Module):\n",
        "    def __init__(self, input_dim=10):\n",
        "        super().__init__()\n",
        "        # Input is the target model's logit vector (size 10 for CIFAR-10) \n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.activation1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.activation2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(32, 1) # 0 Non-Member, 1 Member\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation1(self.fc1(x))\n",
        "        x = self.activation2(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "def prepare_attack_dataset(shadow_models, shadow_indices, full_dataset, device='cuda'):\n",
        " #   Generate (logit vector, membership label) pairs from shadow models\n",
        "\n",
        "    X_attack = []\n",
        "    y_attack = []\n",
        "    \n",
        "    with t.no_grad():\n",
        "        for i, model in enumerate(shadow_models):\n",
        "            model.eval()\n",
        "            train_idx, test_idx = shadow_indices[i]\n",
        "            train_set = set(train_idx.tolist()) if hasattr(train_idx, 'tolist') else set(train_idx)\n",
        "            \n",
        "            # Combine train and test indices for this shadow model\n",
        "            all_shadow_idx = np.concatenate([train_idx, test_idx])\n",
        "            shadow_subset = Subset(full_dataset, all_shadow_idx)\n",
        "            shadow_loader = DataLoader(shadow_subset, batch_size=128, shuffle=False, num_workers=0)\n",
        "            \n",
        "            # Get predictions for shadow model's data\n",
        "            all_preds = []\n",
        "            for inputs, _ in shadow_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                outputs = model(inputs)\n",
        "                preds = F.softmax(outputs, dim=1)\n",
        "                all_preds.append(preds.cpu())\n",
        "\n",
        "            all_preds = t.cat(all_preds)\n",
        "            \n",
        "            # Label based on membership\n",
        "            for j, idx in enumerate(all_shadow_idx):\n",
        "                pred_vector = all_preds[j]\n",
        "                label = 1.0 if idx in train_set else 0.0\n",
        "                \n",
        "                X_attack.append(pred_vector)\n",
        "                y_attack.append(label)\n",
        "                \n",
        "    X_attack = t.stack(X_attack)\n",
        "    y_attack = t.tensor(y_attack).unsqueeze(1)\n",
        "    \n",
        "    return X_attack, y_attack\n",
        "\n",
        "def train_attack_model(X_attack, y_attack, save_path=None, epochs=20, device='cuda'):\n",
        "\n",
        "    attack_model = AttackNet().to(device)\n",
        "    optimizer = t.optim.Adam(attack_model.parameters(), lr=0.001)\n",
        "    criterion = nn.BCELoss() # Binary Cross Entropy Loss\n",
        "    \n",
        "    dataset = t.utils.data.TensorDataset(X_attack, y_attack)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "    \n",
        "    print(\"Training Attack Model...\")\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = attack_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    if save_path:\n",
        "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "        t.save(attack_model.state_dict(), save_path)\n",
        "        print(f\"Saved attack model\") \n",
        "             \n",
        "    return attack_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_mia_attack(target_model, attack_model, train_loader, test_loader, device, is_mpc=False):\n",
        "    \"\"\"\n",
        "    Runs the trained attack model on the target model's members (train_loader) \n",
        "    and non-members (test_loader) to calculate attack accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    target_model.eval()\n",
        "    attack_model.eval()\n",
        "    \n",
        "    def get_target_preds(loader, is_member):\n",
        "        preds = []\n",
        "        labels = []\n",
        "        \n",
        "        for inputs, _ in loader:\n",
        "\n",
        "            if is_mpc:\n",
        "                # MPC Path: Encrypt -> Forward -> Decrypt\n",
        "                x_enc = crypten.cryptensor(inputs)\n",
        "                output_enc = target_model(x_enc)\n",
        "                output_plain = output_enc.get_plain_text()\n",
        "                \n",
        "                # Apply softmax on plaintext for the attack features\n",
        "                batch_preds = F.softmax(output_plain, dim=1)\n",
        "\n",
        "            else:\n",
        "                # Plaintext Path\n",
        "                inputs = inputs.to(device)\n",
        "                with t.no_grad():\n",
        "                    outputs = target_model(inputs)\n",
        "                    batch_preds = F.softmax(outputs, dim=1)\n",
        "            \n",
        "            preds.append(batch_preds.cpu())\n",
        "            labels.extend([1.0 if is_member else 0.0] * inputs.size(0))\n",
        "        return t.cat(preds), t.tensor(labels).unsqueeze(1)\n",
        "    \n",
        "    member_preds, member_labels = get_target_preds(train_loader, is_member=True)\n",
        "    non_member_preds, non_member_labels = get_target_preds(test_loader, is_member=False)\n",
        "    \n",
        "    all_preds = t.cat([member_preds, non_member_preds])\n",
        "    all_labels = t.cat([member_labels, non_member_labels])\n",
        "    \n",
        "    with t.no_grad():\n",
        "        attack_probs = attack_model(all_preds.to(device))\n",
        "        attack_preds = (attack_probs > 0.5).float().cpu()\n",
        "        \n",
        "    correct = (attack_preds == all_labels).sum().item()\n",
        "    total = all_labels.size(0)\n",
        "    accuracy = 100.0 * correct / total\n",
        "    \n",
        "    tp = ((attack_preds == 1) & (all_labels == 1)).sum().item()\n",
        "    fp = ((attack_preds == 1) & (all_labels == 0)).sum().item()\n",
        "    fn = ((attack_preds == 0) & (all_labels == 1)).sum().item()\n",
        "    \n",
        "    precision = 100.0 * tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = 100.0 * tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    \n",
        "    print(f\"MIA Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"MIA Precision (Member): {precision:.2f}%\")\n",
        "    print(f\"MIA Recall (Member): {recall:.2f}%\")\n",
        "    \n",
        "    return accuracy, precision, recall\n",
        "    \n",
        "def evaluate_accuracy_loss(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with t.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            predictions = outputs.argmax(dim=1)\n",
        "            correct += (predictions == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    avg_loss = running_loss / len(test_loader)\n",
        "    accuracy = 100.0 * correct / total\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _setup_dirs():\n",
        "    ARTIFACTS_DIR = './artifacts'\n",
        "    DIRS = {\n",
        "        'target_plain': os.path.join(ARTIFACTS_DIR, 'plaintext_targets'),\n",
        "        'target_mpc':   os.path.join(ARTIFACTS_DIR, 'mpc_checkpoints'),\n",
        "        'shadows':      os.path.join(ARTIFACTS_DIR, 'shadow_models'),\n",
        "        'attacks':      os.path.join(ARTIFACTS_DIR, 'attack_models'),\n",
        "    }\n",
        "    # Create all directories\n",
        "    for d in DIRS.values():\n",
        "        os.makedirs(d, exist_ok=True)\n",
        "\n",
        "    print(f\"Artifacts will be saved to: {ARTIFACTS_DIR}\")\n",
        "    return DIRS\n",
        "\n",
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    plaintext_epochs: int = 120\n",
        "    mpc_epochs: int = 80\n",
        "    shadow_epochs: int = 10\n",
        "    attack_epochs: int = 20\n",
        "    num_shadow_models: int = 5\n",
        "    target_train_size: int = 10000\n",
        "    batch_size: int = 128\n",
        "    mpc_batch_size: int = 32\n",
        "    learning_rate: float = 1e-2\n",
        "    shadow_pool_ratio: float = 0.5\n",
        "    seed: int = 42\n",
        "    num_workers: int = 2\n",
        "    checkpoint_dir: str = './weights_mpc'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artifacts will be saved to: ./artifacts\n",
            "Device: cpu\n",
            "Config: ExperimentConfig(plaintext_epochs=120, mpc_epochs=80, shadow_epochs=10, attack_epochs=20, num_shadow_models=5, target_train_size=10000, batch_size=128, mpc_batch_size=32, learning_rate=0.01, shadow_pool_ratio=0.5, seed=42, num_workers=2, checkpoint_dir='./weights_mpc')\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Target model training set size: 10000\n",
            "Target model testing set size:  10000\n",
            "Shadow model training pool size:  20000\n"
          ]
        }
      ],
      "source": [
        "cfg = ExperimentConfig()\n",
        "DIRS = _setup_dirs()\n",
        "device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Config: {cfg}\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616)) # Standard CIFAR-10 values\n",
        "])\n",
        "\n",
        "full_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
        "\n",
        "target_train_idx, target_test_idx, shadow_pool_idx = partition_dataset_for_mia(\n",
        "    full_dataset=full_dataset,\n",
        "    target_train_size=cfg.target_train_size,\n",
        "    shadow_pool_ratio=cfg.shadow_pool_ratio,\n",
        "    seed=cfg.seed\n",
        ")\n",
        "\n",
        "target_train_subset = Subset(full_dataset, target_train_idx)\n",
        "target_test_subset = Subset(full_dataset, target_test_idx)\n",
        "\n",
        "target_train_loader = DataLoader(target_train_subset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
        "target_test_loader = DataLoader(target_test_subset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
        "target_train_loader_mpc = DataLoader(target_train_subset, batch_size=cfg.mpc_batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() # Does using this for evaluations make sense? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing model: PlainTextCNN_Sigmoid\n",
            "Starting Plaintext Training for 120 epochs...\n",
            "Epoch 1/120 complete: Loss: 2.3226 | Acc: 9.57%\n",
            "Epoch 2/120 complete: Loss: 2.3248 | Acc: 10.10%\n",
            "Epoch 3/120 complete: Loss: 2.3157 | Acc: 10.12%\n",
            "Epoch 4/120 complete: Loss: 2.3068 | Acc: 10.47%\n",
            "Epoch 5/120 complete: Loss: 2.3028 | Acc: 10.99%\n",
            "Epoch 6/120 complete: Loss: 2.2950 | Acc: 11.19%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[37], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(DIRS[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget_plain\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m target_model, _ \u001b[38;5;241m=\u001b[39m \u001b[43mplaintext_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplaintext_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msave_path\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m evaluate_accuracy_loss(target_model, test_loader, criterion, device)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Test Acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[30], line 25\u001b[0m, in \u001b[0;36mplaintext_train_model\u001b[0;34m(model, train_loader, num_epochs, lr, device, save_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     28\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
            "File \u001b[0;32m~/anaconda3/envs/crypten_env/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/crypten_env/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Train plaintext targets, shadows, and attack models\n",
        "\n",
        "results = {}\n",
        "attack_models = {}\n",
        "\n",
        "for name, model_class in PLAINTEXT_MODELS.items():\n",
        "    print(f\"Processing model: {name}\")\n",
        "\n",
        "    model = model_class(num_classes=10).to(device)\n",
        "    save_path = os.path.join(DIRS['target_plain'], f\"{name}.pt\")\n",
        "    \n",
        "    target_model, _ = plaintext_train_model(\n",
        "        model, target_train_loader, cfg.plaintext_epochs, \n",
        "        lr=cfg.learning_rate, device=device, save_path=save_path\n",
        "    )\n",
        "    \n",
        "    test_loss, test_acc = evaluate_accuracy_loss(target_model, test_loader, criterion, device)\n",
        "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
        "    \n",
        "    print(f\"Training {cfg.num_shadow_models} shadow models...\")\n",
        "    shadow_models, shadow_indices = train_shadow_models(\n",
        "        num_shadows=cfg.num_shadow_models,\n",
        "        model_class=model_class,\n",
        "        full_dataset=full_dataset,\n",
        "        shadow_pool_indices=shadow_pool_idx,\n",
        "        model_name=name,\n",
        "        base_dir=DIRS['shadows'],\n",
        "        num_epochs=cfg.shadow_epochs,\n",
        "        device=device\n",
        "    )\n",
        "    \n",
        "    X_attack, y_attack = prepare_attack_dataset(shadow_models, shadow_indices, full_dataset, device=device)\n",
        "    \n",
        "    arch_key = name.replace('PlainText', '')\n",
        "    attack_path = os.path.join(DIRS['attacks'], f\"attack_{arch_key}.pt\")\n",
        "    \n",
        "    attack_model = train_attack_model(\n",
        "        X_attack, y_attack, epochs=cfg.attack_epochs, \n",
        "        device=device, save_path=attack_path\n",
        "    )\n",
        "    attack_models[arch_key] = attack_model\n",
        "    \n",
        "    acc, prec, rec = evaluate_mia_attack(\n",
        "        target_model=target_model,\n",
        "        attack_model=attack_model,\n",
        "        train_loader=target_train_loader,\n",
        "        test_loader=target_test_loader,\n",
        "        device=device,\n",
        "        is_mpc=False\n",
        "    )\n",
        "    results[name] = {\n",
        "        'mia_accuracy': acc, 'mia_precision': prec, 'mia_recall': rec,\n",
        "        'test_loss': test_loss, 'test_accuracy': test_acc, 'is_mpc': False\n",
        "    }\n",
        "\n",
        "for name, model_class in MPC_MODELS.items():\n",
        "    print(f\"Processing model: {name}\")\n",
        "    \n",
        "    model = model_class(num_classes=10)\n",
        "    target_model, _ = mpc_train_model(\n",
        "        model, target_train_loader_mpc, cfg.mpc_epochs, lr=cfg.learning_rate,\n",
        "        model_name=name, checkpoint_dir=DIRS['target_mpc']\n",
        "    )\n",
        "    \n",
        "    target_model.decrypt()\n",
        "    test_loss, test_acc = evaluate_accuracy_loss(target_model, test_loader, criterion, device='cpu')\n",
        "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
        "    target_model.encrypt()\n",
        "    \n",
        "    arch_key = name.replace('Mpc', '')\n",
        "    if arch_key in attack_models:\n",
        "        print(f\"Reusing attack model from PlainText{arch_key}\")\n",
        "        attack_model = attack_models[arch_key]\n",
        "        \n",
        "        acc, prec, rec = evaluate_mia_attack(\n",
        "            target_model=target_model,\n",
        "            attack_model=attack_model,\n",
        "            train_loader=target_train_loader_mpc,\n",
        "            test_loader=target_test_loader,\n",
        "            device=device,\n",
        "            is_mpc=True\n",
        "        )\n",
        "        results[name] = {\n",
        "            'mia_accuracy': acc, 'mia_precision': prec, 'mia_recall': rec,\n",
        "            'test_loss': test_loss, 'test_accuracy': test_acc, 'is_mpc': True\n",
        "        }\n",
        "\n",
        "print(f\"\\n{'='*90}\")\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(f\"{'='*90}\")\n",
        "print(f\"{'Model':<25} {'Type':<10} {'Test Acc':<10} {'Test Loss':<10} {'MIA Acc':<10} {'MIA Prec':<10} {'MIA Rec':<10}\")\n",
        "print(\"-\" * 90)\n",
        "for name, res in sorted(results.items()):\n",
        "    t_type = 'MPC' if res['is_mpc'] else 'Plaintext'\n",
        "    print(f\"{name:<25} {t_type:<10} {res['test_accuracy']:<10.2f} {res['test_loss']:<10.4f} {res['mia_accuracy']:<10.2f} {res['mia_precision']:<10.2f} {res['mia_recall']:<10.2f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "crypten_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
