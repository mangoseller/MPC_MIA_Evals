{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "zUHL7zZmSJTQ"
      },
      "outputs": [],
      "source": [
        "import torch as t\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "from tqdm.notebook import trange, tqdm\n",
        "import os\n",
        "from scipy import stats\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import crypten\n",
        "import crypten.nn as cnn\n",
        "import time\n",
        "from copy import deepcopy\n",
        "from functools import partial\n",
        "from dataclasses import dataclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "B5RpGIyKmouq"
      },
      "outputs": [],
      "source": [
        "def load_dataset(batch_size=128, num_workers=2):\n",
        "    temp_dataset = datasets.CIFAR10(root='./data', train=True, download=True,\n",
        "                                     transform=transforms.ToTensor())\n",
        "    temp_loader = DataLoader(temp_dataset, batch_size=batch_size, num_workers=num_workers)\n",
        "\n",
        "    channels_sum = t.zeros(3)\n",
        "    channels_squared_sum = t.zeros(3)\n",
        "    num_pixels = 0\n",
        "\n",
        "    for images, _ in temp_loader:\n",
        "        channels_sum += images.sum(dim=[0, 2, 3])\n",
        "        channels_squared_sum += (images ** 2).sum(dim=[0, 2, 3])\n",
        "        num_pixels += images.size(0) * images.size(2) * images.size(3)\n",
        "\n",
        "    mean = channels_sum / num_pixels\n",
        "    std = ((channels_squared_sum / num_pixels) - (mean ** 2)) ** 0.5\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean.tolist(), std.tolist())\n",
        "    ])\n",
        "\n",
        "    train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "    test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "k9AfmmWtoOyV"
      },
      "outputs": [],
      "source": [
        "\"\"\"Plaintext models with parameterized activation functions.\n",
        "Pass activation_fn for hidden layers, pass output_fn to \n",
        "apply an activation after the final layer\"\"\"\n",
        "\n",
        "class PlainTextCNN(nn.Module):\n",
        "    def __init__(self, num_classes=10, activation_fn=nn.Sigmoid, output_fn=None):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.activation1 = activation_fn()\n",
        "        self.pool1 = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.activation2 = activation_fn()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
        "        self.activation3 = activation_fn()\n",
        "        self.fc2 = nn.Linear(512, num_classes)\n",
        "\n",
        "        layers = [\n",
        "            self.conv1,\n",
        "            self.activation1,\n",
        "            self.pool1,\n",
        "            self.conv2,\n",
        "            self.activation2,\n",
        "            self.pool2,\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.activation3,\n",
        "            self.fc2\n",
        "        ]\n",
        "        if output_fn is not None:\n",
        "            self.output_activation = output_fn()\n",
        "            layers.append(self.output_activation)\n",
        "        \n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "class PlainTextMLP(nn.Module):\n",
        "    \n",
        "    def __init__(self, num_classes=10, activation_fn=nn.Sigmoid, output_fn=None):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(3072, 512)\n",
        "        self.activation1 = activation_fn()\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.activation2 = activation_fn()\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "        layers = [\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.activation1,\n",
        "            self.fc2,\n",
        "            self.activation2,\n",
        "            self.fc3\n",
        "        ]\n",
        "        if output_fn is not None:\n",
        "            self.output_activation = output_fn()\n",
        "            layers.append(self.output_activation)\n",
        "        \n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class PlainTextLeNet(nn.Module):\n",
        "    def __init__(self, num_classes=10, activation_fn=nn.Sigmoid, output_fn=None):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, kernel_size=5, padding=2)\n",
        "        self.bn1 = nn.BatchNorm2d(6)\n",
        "        self.activation1 = activation_fn()\n",
        "        self.pool1 = nn.AvgPool2d(2, 2)\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "        self.activation2 = activation_fn()\n",
        "        self.pool2 = nn.AvgPool2d(2, 2)\n",
        "        \n",
        "        self.flatten = nn.Flatten()\n",
        "        \n",
        "        self.fc1 = nn.Linear(16 * 6 * 6, 120)\n",
        "        self.bn3 = nn.BatchNorm1d(120)\n",
        "        self.activation3 = activation_fn()\n",
        "        \n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.bn4 = nn.BatchNorm1d(84)\n",
        "        self.activation4 = activation_fn()\n",
        "        \n",
        "        self.fc3 = nn.Linear(84, num_classes)\n",
        "\n",
        "        layers = [\n",
        "            self.conv1,\n",
        "            self.bn1,\n",
        "            self.activation1,\n",
        "            self.pool1,\n",
        "            self.conv2,\n",
        "            self.bn2,\n",
        "            self.activation2,\n",
        "            self.pool2,\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.bn3,\n",
        "            self.activation3,\n",
        "            self.fc2,\n",
        "            self.bn4,\n",
        "            self.activation4,\n",
        "            self.fc3\n",
        "        ]\n",
        "        if output_fn is not None:\n",
        "            self.output_activation = output_fn()\n",
        "            layers.append(self.output_activation)\n",
        "        \n",
        "        self.network = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sigmoid variants\n",
        "PlainTextCNN_Sigmoid = partial(PlainTextCNN, activation_fn=nn.Sigmoid)\n",
        "PlainTextMLP_Sigmoid = partial(PlainTextMLP, activation_fn=nn.Sigmoid)\n",
        "PlainTextLeNet_Sigmoid = partial(PlainTextLeNet, activation_fn=nn.Sigmoid)\n",
        "\n",
        "# Tanh variants\n",
        "PlainTextCNN_Tanh = partial(PlainTextCNN, activation_fn=nn.Tanh)\n",
        "PlainTextMLP_Tanh = partial(PlainTextMLP, activation_fn=nn.Tanh)\n",
        "PlainTextLeNet_Tanh = partial(PlainTextLeNet, activation_fn=nn.Tanh)\n",
        "\n",
        "# GELU variants\n",
        "PlainTextCNN_GELU = partial(PlainTextCNN, activation_fn=nn.GELU)\n",
        "PlainTextMLP_GELU = partial(PlainTextMLP, activation_fn=nn.GELU)\n",
        "PlainTextLeNet_GELU = partial(PlainTextLeNet, activation_fn=nn.GELU)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "EnLKsJQ8qjqK"
      },
      "outputs": [],
      "source": [
        "def plaintext_train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    pbar = tqdm(train_loader, desc='Training', leave=False)\n",
        "    for inputs, targets in pbar:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        predictions = outputs.argmax(dim=1)\n",
        "        correct += (predictions == targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "\n",
        "        pbar.set_postfix({'loss': running_loss / (pbar.n + 1), 'acc': 100.0 * correct / total})\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100.0 * correct / total\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "\n",
        "def plaintext_train_model(model, train_loader, num_epochs=10, lr=0.001, device='cuda'):\n",
        "    model = model.to(device)\n",
        "    optimizer = t.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = plaintext_train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {train_loss:.4f} | Acc: {train_acc:.2f}%')\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def train_plaintext_models(epochs=10):\n",
        "    device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
        "\n",
        "    train_loader, test_loader = load_dataset(batch_size=128, num_workers=2)\n",
        "\n",
        "    models = {\n",
        "        'PlainTextCNN': PlainTextCNN(num_classes=10),\n",
        "        'PlainTextMLP': PlainTextMLP(num_classes=10),\n",
        "        'PlainTextLeNet': PlainTextLeNet(num_classes=10)\n",
        "    }\n",
        "\n",
        "    os.makedirs('./weights', exist_ok=True)\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        print(f'\\nTraining {model_name}...')\n",
        "        trained_model, history = plaintext_train_model(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            num_epochs=epochs,\n",
        "            lr=1e-3,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        final_weights_path = f'./weights/{model_name}_final.pt'\n",
        "        t.save(trained_model.state_dict(), final_weights_path)\n",
        "        print(f'Final weights saved: {final_weights_path}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMQrARNevlfE",
        "outputId": "10374f39-700c-4391-cbdc-eab6a11a8590"
      },
      "outputs": [],
      "source": [
        "def evaluate_accuracy_loss(model, test_loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with t.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            predictions = outputs.argmax(dim=1)\n",
        "            correct += (predictions == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    avg_loss = running_loss / len(test_loader)\n",
        "    accuracy = 100.0 * correct / total\n",
        "\n",
        "    return avg_loss, accuracy\n",
        "\n",
        "def load_model_from_weights(model_class, weights_path, num_classes=10, device='cuda'):\n",
        "    model = model_class(num_classes=num_classes)\n",
        "    model.load_state_dict(t.load(weights_path, map_location=device, weights_only=True))\n",
        "    model = model.to(device)\n",
        "    print(f'Loaded weights from: {weights_path}')\n",
        "    return model\n",
        "\n",
        "def continue_training(model, train_loader, num_epochs=10, lr=0.001, device='cuda',\n",
        "                      start_epoch=0, save_path=None):\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = t.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = plaintext_train_epoch(model, train_loader, optimizer, criterion, device)\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "\n",
        "        display_epoch = start_epoch + epoch + 1\n",
        "        print(f'Epoch {display_epoch} - Loss: {train_loss:.4f} | Acc: {train_acc:.2f}%')\n",
        "\n",
        "    if save_path is not None:\n",
        "        t.save(model.state_dict(), save_path)\n",
        "        print(f'Weights saved: {save_path}')\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def load_and_continue_training(model_class, weights_path, train_loader, num_epochs=10,\n",
        "                                lr=0.01, device='cuda', start_epoch=0, save_path=None):\n",
        "\n",
        "    model = load_model_from_weights(model_class, weights_path, device=device)\n",
        "    model, history = continue_training(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        num_epochs=num_epochs,\n",
        "        lr=lr,\n",
        "        device=device,\n",
        "        start_epoch=start_epoch,\n",
        "        save_path=save_path\n",
        "    )\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"CrypTen models with parameterized activation functions.\n",
        "   CrypTen requires manual flattening and uses cnn.* modules.\n",
        "   Pass output_fn to apply an activation after the final layer\"\"\"\n",
        "   \n",
        "# TODO: Investigate how these activation functions are being approximated exactly, and replace them where required\n",
        "class MpcFlatten(cnn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.flatten(start_dim=1)\n",
        "\n",
        "class MpcCNN(cnn.Module):\n",
        "    def __init__(self, num_classes=10, activation_fn=cnn.Sigmoid, output_fn=None):\n",
        "        super().__init__()\n",
        "        self.conv1 = cnn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.activation1 = activation_fn()\n",
        "        # Note that MaxPool2d is significantly more expensive in MPC\n",
        "        self.pool1 = cnn.MaxPool2d(2, 2)\n",
        "        self.conv2 = cnn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.activation2 = activation_fn()\n",
        "        self.pool2 = cnn.MaxPool2d(2, 2)\n",
        "        \n",
        "        self.flatten = MpcFlatten()\n",
        "        \n",
        "        self.fc1 = cnn.Linear(64 * 8 * 8, 512)\n",
        "        self.activation3 = activation_fn()\n",
        "        self.fc2 = cnn.Linear(512, num_classes)\n",
        "\n",
        "        layers = [\n",
        "            self.conv1,\n",
        "            self.activation1,\n",
        "            self.pool1,\n",
        "            self.conv2,\n",
        "            self.activation2,\n",
        "            self.pool2,\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.activation3,\n",
        "            self.fc2\n",
        "        ]\n",
        "        if output_fn is not None:\n",
        "            self.output_activation = output_fn()\n",
        "            layers.append(self.output_activation)\n",
        "        \n",
        "        self.network = cnn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "class MpcMLP(cnn.Module):\n",
        "    def __init__(self, num_classes=10, activation_fn=cnn.Sigmoid, output_fn=None):\n",
        "        super().__init__()\n",
        "        self.flatten = MpcFlatten()\n",
        "        self.fc1 = cnn.Linear(3072, 512)\n",
        "        self.activation1 = activation_fn()\n",
        "        self.fc2 = cnn.Linear(512, 256)\n",
        "        self.activation2 = activation_fn()\n",
        "        self.fc3 = cnn.Linear(256, num_classes)\n",
        "\n",
        "        layers = [\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.activation1,\n",
        "            self.fc2,\n",
        "            self.activation2,\n",
        "            self.fc3\n",
        "        ]\n",
        "        if output_fn is not None:\n",
        "            self.output_activation = output_fn()\n",
        "            layers.append(self.output_activation)\n",
        "        \n",
        "        self.network = cnn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)\n",
        "\n",
        "\n",
        "class MpcLeNet(cnn.Module):\n",
        "    def __init__(self, num_classes=10, activation_fn=cnn.Sigmoid, output_fn=None):\n",
        "        super().__init__()\n",
        "        self.conv1 = cnn.Conv2d(3, 6, kernel_size=5, padding=2)\n",
        "        self.bn1 = cnn.BatchNorm2d(6)\n",
        "        self.activation1 = activation_fn()\n",
        "        self.pool1 = cnn.AvgPool2d(2, 2) # AvgPool is efficient in MPC\n",
        "        \n",
        "        self.conv2 = cnn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.bn2 = cnn.BatchNorm2d(16)\n",
        "        self.activation2 = activation_fn()\n",
        "        self.pool2 = cnn.AvgPool2d(2, 2)\n",
        "        \n",
        "        self.flatten = MpcFlatten()\n",
        "        \n",
        "        self.fc1 = cnn.Linear(16 * 6 * 6, 120)\n",
        "        self.bn3 = cnn.BatchNorm1d(120)\n",
        "        self.activation3 = activation_fn()\n",
        "        \n",
        "        self.fc2 = cnn.Linear(120, 84)\n",
        "        self.bn4 = cnn.BatchNorm1d(84)\n",
        "        self.activation4 = activation_fn()\n",
        "        \n",
        "        self.fc3 = cnn.Linear(84, num_classes)\n",
        "\n",
        "        layers = [\n",
        "            self.conv1,\n",
        "            self.bn1,\n",
        "            self.activation1,\n",
        "            self.pool1,\n",
        "            self.conv2,\n",
        "            self.bn2,\n",
        "            self.activation2,\n",
        "            self.pool2,\n",
        "            self.flatten,\n",
        "            self.fc1,\n",
        "            self.bn3,\n",
        "            self.activation3,\n",
        "            self.fc2,\n",
        "            self.bn4,\n",
        "            self.activation4,\n",
        "            self.fc3\n",
        "        ]\n",
        "        if output_fn is not None:\n",
        "            self.output_activation = output_fn()\n",
        "            layers.append(self.output_activation)\n",
        "        \n",
        "        self.network = cnn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MpcTanh(cnn.Module):\n",
        "    # Wrapper for Tanh activation in CrypTen.\n",
        "    def forward(self, x):\n",
        "        return x.tanh()\n",
        "\n",
        "MpcCNN_Sigmoid = partial(MpcCNN, activation_fn=cnn.Sigmoid)\n",
        "MpcMLP_Sigmoid = partial(MpcMLP, activation_fn=cnn.Sigmoid)\n",
        "MpcLeNet_Sigmoid = partial(MpcLeNet, activation_fn=cnn.Sigmoid)\n",
        "\n",
        "\n",
        "MpcCNN_Tanh = partial(MpcCNN, activation_fn=MpcTanh)\n",
        "MpcMLP_Tanh = partial(MpcMLP, activation_fn=MpcTanh)\n",
        "MpcLeNet_Tanh = partial(MpcLeNet, activation_fn=MpcTanh)\n",
        "\n",
        "\n",
        "MpcCNN_ReLU = partial(MpcCNN, activation_fn=cnn.ReLU)\n",
        "MpcMLP_ReLU = partial(MpcMLP, activation_fn=cnn.ReLU)\n",
        "MpcLeNet_ReLU = partial(MpcLeNet, activation_fn=cnn.ReLU)\n",
        "\n",
        "\n",
        "MPC_MODELS = {\n",
        "    'MpcCNN_Sigmoid': MpcCNN_Sigmoid,\n",
        "    'MpcCNN_Tanh': MpcCNN_Tanh,\n",
        "    'MpcCNN_ReLU': MpcCNN_ReLU,\n",
        "    'MpcMLP_Sigmoid': MpcMLP_Sigmoid,\n",
        "    'MpcMLP_Tanh': MpcMLP_Tanh,\n",
        "    'MpcMLP_ReLU': MpcMLP_ReLU,\n",
        "    'MpcLeNet_Sigmoid': MpcLeNet_Sigmoid,\n",
        "    'MpcLeNet_Tanh': MpcLeNet_Tanh,\n",
        "    'MpcLeNet_ReLU': MpcLeNet_ReLU,\n",
        "}\n",
        "\n",
        " \n",
        "PLAINTEXT_MODELS = {\n",
        "    'PlainTextCNN_Sigmoid': PlainTextCNN_Sigmoid,\n",
        "    'PlainTextCNN_Tanh': PlainTextCNN_Tanh,\n",
        "    'PlainTextCNN_ReLU': partial(PlainTextCNN, activation_fn=nn.ReLU), \n",
        "    'PlainTextMLP_Sigmoid': PlainTextMLP_Sigmoid,\n",
        "    'PlainTextMLP_Tanh': PlainTextMLP_Tanh,\n",
        "    'PlainTextMLP_ReLU': partial(PlainTextMLP, activation_fn=nn.ReLU), \n",
        "    'PlainTextLeNet_Sigmoid': PlainTextLeNet_Sigmoid,\n",
        "    'PlainTextLeNet_Tanh': PlainTextLeNet_Tanh,\n",
        "    'PlainTextLeNet_ReLU': partial(PlainTextLeNet, activation_fn=nn.ReLU), \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "def mpc_train_epoch(model, train_loader, optimizer, criterion, device, num_classes=10):\n",
        "\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    pbar = tqdm(train_loader, desc='MPC Training', leave=False)\n",
        "    \n",
        "    for inputs, targets in pbar:\n",
        "\n",
        "        x_enc = crypten.cryptensor(inputs)\n",
        "        y_one_hot = F.one_hot(targets, num_classes=num_classes).float()\n",
        "        y_enc = crypten.cryptensor(y_one_hot)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output_enc = model(x_enc)\n",
        "        loss_enc = criterion(output_enc, y_enc)\n",
        "        loss_enc.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_val = loss_enc.get_plain_text().item()\n",
        "        running_loss += loss_val\n",
        "        \n",
        "        # Decrypt predictions for accuracy\n",
        "        output_plain = output_enc.get_plain_text()\n",
        "        predictions = output_plain.argmax(dim=1)\n",
        "        correct += (predictions == targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "\n",
        "        current_loss = running_loss / (total / inputs.size(0)) # approximates average loss\n",
        "        current_acc = 100.0 * correct / total\n",
        "        pbar.set_postfix({'loss': f'{current_loss:.4f}', 'acc': f'{current_acc:.2f}%'})\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100.0 * correct / total\n",
        "    \n",
        "    return avg_loss, accuracy\n",
        "    \n",
        "def mpc_train_model(model, train_loader, num_epochs=10, lr=0.001, device='cpu', \n",
        "                    model_name='MpcModel', checkpoint_dir='./weights_mpc'):\n",
        "\n",
        "    if not crypten.is_initialized():\n",
        "        crypten.init()\n",
        "    \n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    \n",
        "    model.encrypt()\n",
        "    model.train()\n",
        "\n",
        "    optimizer = crypten.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    criterion = cnn.CrossEntropyLoss()\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': []\n",
        "    }\n",
        "\n",
        "    # Calculate checkpoint epochs (every 1/5th of training)\n",
        "    checkpoint_interval = max(1, num_epochs // 5)\n",
        "    checkpoint_epochs = set(range(checkpoint_interval, num_epochs + 1, checkpoint_interval))\n",
        "    # Always include final epoch\n",
        "    checkpoint_epochs.add(num_epochs)\n",
        "\n",
        "    print(f\"Starting MPC Training for {num_epochs} epochs...\")\n",
        "    print(f\"Checkpoints at epochs: {sorted(checkpoint_epochs)}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss, train_acc = mpc_train_epoch(\n",
        "            model, \n",
        "            train_loader, \n",
        "            optimizer, \n",
        "            criterion, \n",
        "            device\n",
        "        )\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['train_acc'].append(train_acc)\n",
        "        \n",
        "        elapsed = time.time() - start_time\n",
        "        current_epoch = epoch + 1\n",
        "        print(f'Epoch {current_epoch}/{num_epochs} - Loss: {train_loss:.4f} | Acc: {train_acc:.2f}% | Time: {elapsed:.0f}s')\n",
        "\n",
        "        if current_epoch in checkpoint_epochs:\n",
        "            checkpoint_path = f'{checkpoint_dir}/{model_name}_epoch{current_epoch}.pt'\n",
        "            crypten.save(model.state_dict(), checkpoint_path)\n",
        "            print(f'  -> Checkpoint saved: {checkpoint_path}')\n",
        "\n",
        "    return model, history\n",
        "\n",
        "def train_mpc_models(epochs=10):\n",
        "\n",
        "    train_loader, test_loader = load_dataset(batch_size=32, num_workers=2)\n",
        "\n",
        "    models = {\n",
        "        'MpcCNN': MpcCNN(num_classes=10),\n",
        "        'MpcMLP': MpcMLP(num_classes=10),\n",
        "        'MpcLeNet': MpcLeNet(num_classes=10)\n",
        "    }\n",
        "\n",
        "    os.makedirs('./weights_mpc', exist_ok=True)\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        print(f'\\nTraining {model_name} in MPC...')\n",
        "        \n",
        "        trained_model, history = mpc_train_model(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            num_epochs=epochs,\n",
        "            lr=1e-3,\n",
        "            model_name=model_name,\n",
        "            checkpoint_dir='./weights_mpc'\n",
        "        )\n",
        "        \n",
        "        print(f'{model_name} training complete.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def partition_dataset_for_mia(full_dataset, target_train_size, shadow_pool_ratio=0.5, seed=42):\n",
        "    \n",
        "# Partition dataset into disjoint pools for target model and shadow models\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    dataset_size = len(full_dataset)\n",
        "    all_indices = np.random.permutation(dataset_size)\n",
        "    \n",
        "    # Allocate target training set\n",
        "    target_train_indices = all_indices[:target_train_size]\n",
        "    remaining_indices = all_indices[target_train_size:]\n",
        "    \n",
        "    # Split remaining data between target test set and shadow pool\n",
        "    shadow_pool_size = int(len(remaining_indices) * shadow_pool_ratio)\n",
        "    shadow_pool_indices = remaining_indices[:shadow_pool_size]\n",
        "    target_test_indices = remaining_indices[shadow_pool_size:]\n",
        "    \n",
        "\n",
        "    print(f\"  Target train: {len(target_train_indices)} samples\")\n",
        "    print(f\"  Target test:  {len(target_test_indices)} samples\")\n",
        "    print(f\"  Shadow pool:  {len(shadow_pool_indices)} samples\")\n",
        "    \n",
        "    return target_train_indices, target_test_indices, shadow_pool_indices\n",
        "\n",
        "\n",
        "def train_shadow_models(num_shadows, model_class, full_dataset, shadow_pool_indices, \n",
        "                        num_epochs=10, device='cuda'):\n",
        " #   Trains shadow models on the shadow pool, whichi is disjoint from the target model's training data\n",
        "    \n",
        "    shadow_models = []\n",
        "    shadow_data_indices = []\n",
        "    shadow_pool_size = len(shadow_pool_indices)\n",
        "    split_size = shadow_pool_size // 2  # 50% in, 50% out \n",
        "    \n",
        "    print(f\"Training {num_shadows} shadow models on pool of {shadow_pool_size} samples...\")\n",
        "    print(f\"Each shadow model: {split_size} train, {shadow_pool_size - split_size} test\")\n",
        "    \n",
        "    for i in range(num_shadows):\n",
        "        # Random permutation within the shadow pool (shadow datasets may overlap)\n",
        "        perm = np.random.permutation(shadow_pool_size)\n",
        "        train_local_indices = perm[:split_size]\n",
        "        test_local_indices = perm[split_size:]\n",
        "        \n",
        "        # Map local indices back to full dataset indices\n",
        "        train_indices = shadow_pool_indices[train_local_indices]\n",
        "        test_indices = shadow_pool_indices[test_local_indices]\n",
        "        \n",
        "        train_subset = Subset(full_dataset, train_indices)\n",
        "        train_loader = DataLoader(train_subset, batch_size=128, shuffle=True, num_workers=0)\n",
        "        \n",
        "        shadow_model = model_class(num_classes=10).to(device)\n",
        "        \n",
        "        print(f\"Shadow Model {i+1}/{num_shadows}...\")\n",
        "        plaintext_train_model(\n",
        "            shadow_model, \n",
        "            train_loader, \n",
        "            num_epochs=num_epochs, \n",
        "            lr=1e-3, \n",
        "            device=device\n",
        "        )\n",
        "        shadow_models.append(shadow_model)\n",
        "        shadow_data_indices.append((train_indices, test_indices))\n",
        "        \n",
        "    return shadow_models, shadow_data_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttackNet(nn.Module):\n",
        "    def __init__(self, input_dim=10):\n",
        "        super().__init__()\n",
        "        # Input is the target model's logit vector (size 10 for CIFAR-10) \n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.activation1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.activation2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(32, 1) # 0 Non-Member, 1 Member\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.activation1(self.fc1(x))\n",
        "        x = self.activation2(self.fc2(x))\n",
        "        x = self.sigmoid(self.fc3(x))\n",
        "        return x\n",
        "\n",
        "def prepare_attack_dataset(shadow_models, shadow_indices, full_dataset, device='cuda'):\n",
        " #   Generate (logit vector, membership label) pairs from shadow models\n",
        "\n",
        "    X_attack = []\n",
        "    y_attack = []\n",
        "    \n",
        "    with t.no_grad():\n",
        "        for i, model in enumerate(shadow_models):\n",
        "            model.eval()\n",
        "            train_idx, test_idx = shadow_indices[i]\n",
        "            train_set = set(train_idx.tolist()) if hasattr(train_idx, 'tolist') else set(train_idx)\n",
        "            \n",
        "            # Combine train and test indices for this shadow model\n",
        "            all_shadow_idx = np.concatenate([train_idx, test_idx])\n",
        "            shadow_subset = Subset(full_dataset, all_shadow_idx)\n",
        "            shadow_loader = DataLoader(shadow_subset, batch_size=128, shuffle=False, num_workers=0)\n",
        "            \n",
        "            # Get predictions for shadow model's data\n",
        "            all_preds = []\n",
        "            for inputs, _ in shadow_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                outputs = model(inputs)\n",
        "                preds = F.softmax(outputs, dim=1)\n",
        "                all_preds.append(preds.cpu())\n",
        "\n",
        "            all_preds = t.cat(all_preds)\n",
        "            \n",
        "            # Label based on membership\n",
        "            for j, idx in enumerate(all_shadow_idx):\n",
        "                pred_vector = all_preds[j]\n",
        "                label = 1.0 if idx in train_set else 0.0\n",
        "                \n",
        "                X_attack.append(pred_vector)\n",
        "                y_attack.append(label)\n",
        "                \n",
        "    X_attack = t.stack(X_attack)\n",
        "    y_attack = t.tensor(y_attack).unsqueeze(1)\n",
        "    \n",
        "    return X_attack, y_attack\n",
        "\n",
        "def train_attack_model(X_attack, y_attack, epochs=20, device='cuda'):\n",
        "\n",
        "    attack_model = AttackNet().to(device)\n",
        "    optimizer = t.optim.Adam(attack_model.parameters(), lr=0.001)\n",
        "    criterion = nn.BCELoss() # Binary Cross Entropy Loss\n",
        "    \n",
        "    dataset = t.utils.data.TensorDataset(X_attack, y_attack)\n",
        "    loader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
        "    \n",
        "    print(\"Training Attack Model...\")\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, targets in loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = attack_model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "    return attack_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_mia_attack(target_model, attack_model, train_loader, test_loader, device, is_mpc=False):\n",
        "    \"\"\"\n",
        "    Runs the trained attack model on the target model's members (train_loader) \n",
        "    and non-members (test_loader) to calculate attack accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    target_model.eval()\n",
        "    attack_model.eval()\n",
        "    \n",
        "    def get_target_preds(loader, is_member):\n",
        "        preds = []\n",
        "        labels = []\n",
        "        \n",
        "        for inputs, _ in loader:\n",
        "\n",
        "            if is_mpc:\n",
        "                # MPC Path: Encrypt -> Forward -> Decrypt\n",
        "                x_enc = crypten.cryptensor(inputs)\n",
        "                output_enc = target_model(x_enc)\n",
        "                output_plain = output_enc.get_plain_text()\n",
        "                \n",
        "                # Apply softmax on plaintext for the attack features\n",
        "                batch_preds = F.softmax(output_plain, dim=1)\n",
        "\n",
        "            else:\n",
        "                # Plaintext Path\n",
        "                inputs = inputs.to(device)\n",
        "                with t.no_grad():\n",
        "                    outputs = target_model(inputs)\n",
        "                    batch_preds = F.softmax(outputs, dim=1)\n",
        "            \n",
        "            preds.append(batch_preds.cpu())\n",
        "            labels.extend([1.0 if is_member else 0.0] * inputs.size(0))\n",
        "        return t.cat(preds), t.tensor(labels).unsqueeze(1)\n",
        "\n",
        "    print(f\"Collecting predictions from {'MPC' if is_mpc else 'Plaintext'} Target...\")\n",
        "    \n",
        "    member_preds, member_labels = get_target_preds(train_loader, is_member=True)\n",
        "    non_member_preds, non_member_labels = get_target_preds(test_loader, is_member=False)\n",
        "    \n",
        "    all_preds = t.cat([member_preds, non_member_preds])\n",
        "    all_labels = t.cat([member_labels, non_member_labels])\n",
        "    \n",
        "    with t.no_grad():\n",
        "        attack_probs = attack_model(all_preds.to(device))\n",
        "        attack_preds = (attack_probs > 0.5).float().cpu()\n",
        "        \n",
        "    correct = (attack_preds == all_labels).sum().item()\n",
        "    total = all_labels.size(0)\n",
        "    accuracy = 100.0 * correct / total\n",
        "    \n",
        "    tp = ((attack_preds == 1) & (all_labels == 1)).sum().item()\n",
        "    fp = ((attack_preds == 1) & (all_labels == 0)).sum().item()\n",
        "    fn = ((attack_preds == 0) & (all_labels == 1)).sum().item()\n",
        "    \n",
        "    precision = 100.0 * tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = 100.0 * tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    \n",
        "    print(f\"MIA Accuracy: {accuracy:.2f}%\")\n",
        "    print(f\"MIA Precision (Member): {precision:.2f}%\")\n",
        "    print(f\"MIA Recall (Member): {recall:.2f}%\")\n",
        "    \n",
        "    return accuracy, precision, recall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cpu\n",
            "Config: ExperimentConfig(plaintext_epochs=120, mpc_epochs=1, shadow_epochs=10, attack_epochs=20, num_shadow_models=5, target_train_size=10000, batch_size=2, mpc_batch_size=32, learning_rate=0.01, shadow_pool_ratio=0.5, seed=42, num_workers=2, checkpoint_dir='./weights_mpc')\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "  Target train: 10000 samples\n",
            "  Target test:  20000 samples\n",
            "  Shadow pool:  20000 samples\n",
            "Processing model: MpcCNN_Sigmoid\n",
            "Starting MPC Training for 1 epochs...\n",
            "Checkpoints at epochs: [1]\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e14fe16efddc4e95a1b5956cae3aed2c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "MPC Training:   0%|          | 0/313 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "@dataclass\n",
        "class ExperimentConfig:\n",
        "    plaintext_epochs: int = 120\n",
        "    mpc_epochs: int = 80\n",
        "    shadow_epochs: int = 10\n",
        "    attack_epochs: int = 20\n",
        "    num_shadow_models: int = 5\n",
        "    target_train_size: int = 10000\n",
        "    batch_size: int = 128\n",
        "    mpc_batch_size: int = 32\n",
        "    learning_rate: float = 1e-2\n",
        "    shadow_pool_ratio: float = 0.5\n",
        "    seed: int = 42\n",
        "    num_workers: int = 2\n",
        "    checkpoint_dir: str = './weights_mpc'\n",
        "\n",
        "cfg = ExperimentConfig(batch_size=2, mpc_epochs=1)\n",
        "\n",
        "\n",
        "device = 'cuda' if t.cuda.is_available() else 'cpu'\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Config: {cfg}\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "])\n",
        "\n",
        "full_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
        "\n",
        "target_train_idx, target_test_idx, shadow_pool_idx = partition_dataset_for_mia(\n",
        "    full_dataset=full_dataset,\n",
        "    target_train_size=cfg.target_train_size,\n",
        "    shadow_pool_ratio=cfg.shadow_pool_ratio,\n",
        "    seed=cfg.seed\n",
        ")\n",
        "\n",
        "target_train_subset = Subset(full_dataset, target_train_idx)\n",
        "target_test_subset = Subset(full_dataset, target_test_idx)\n",
        "\n",
        "target_train_loader = DataLoader(target_train_subset, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
        "target_test_loader = DataLoader(target_test_subset, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers)\n",
        "target_train_loader_mpc = DataLoader(target_train_subset, batch_size=cfg.mpc_batch_size, shuffle=True, num_workers=cfg.num_workers)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Train plaintext targets, shadows, and attack models\n",
        "\n",
        "results = {}\n",
        "attack_models = {}  # Store attack models for reuse with MPC\n",
        "\n",
        "for name, model_class in PLAINTEXT_MODELS.items():\n",
        "    print(f\"Processing model: {name}\")\n",
        "\n",
        "    model = model_class(num_classes=10).to(device)\n",
        "    target_model, _ = plaintext_train_model(model, target_train_loader, cfg.plaintext_epochs, lr=cfg.learning_rate, device=device)\n",
        "    \n",
        "    # Evaluate model accuracy/loss on test set\n",
        "    test_loss, test_acc = evaluate_accuracy_loss(target_model, test_loader, criterion, device)\n",
        "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
        "    \n",
        "    # Train shadow models\n",
        "    print(f\"Training {cfg.num_shadow_models} shadow models...\")\n",
        "    shadow_models, shadow_indices = train_shadow_models(\n",
        "        num_shadows=cfg.num_shadow_models,\n",
        "        model_class=model_class,\n",
        "        full_dataset=full_dataset,\n",
        "        shadow_pool_indices=shadow_pool_idx,\n",
        "        num_epochs=cfg.shadow_epochs,\n",
        "        device=device\n",
        "    )\n",
        "    \n",
        "    # Train attack model\n",
        "    X_attack, y_attack = prepare_attack_dataset(shadow_models, shadow_indices, full_dataset, device=device)\n",
        "    attack_model = train_attack_model(X_attack, y_attack, epochs=cfg.attack_epochs, device=device)\n",
        "    \n",
        "    # Store attack model for MPC reuse \n",
        "    arch_key = name.replace('PlainText', '')\n",
        "    attack_models[arch_key] = attack_model\n",
        "    \n",
        "    # Evaluate MIA\n",
        "    acc, prec, rec = evaluate_mia_attack(\n",
        "        target_model=target_model,\n",
        "        attack_model=attack_model,\n",
        "        train_loader=target_train_loader,\n",
        "        test_loader=target_test_loader,\n",
        "        device=device,\n",
        "        is_mpc=False\n",
        "    )\n",
        "    results[name] = {\n",
        "        'mia_accuracy': acc, 'mia_precision': prec, 'mia_recall': rec,\n",
        "        'test_loss': test_loss, 'test_accuracy': test_acc, 'is_mpc': False\n",
        "    }\n",
        "\n",
        "# Train MPC targets, reuse attack models from plaintext\n",
        "\n",
        "for name, model_class in MPC_MODELS.items():\n",
        "    print(f\"Processing model: {name}\")\n",
        "    \n",
        "    model = model_class(num_classes=10)\n",
        "    target_model, _ = mpc_train_model(model, target_train_loader_mpc, cfg.mpc_epochs, lr=cfg.learning_rate,\n",
        "                                       model_name=name, checkpoint_dir=cfg.checkpoint_dir)\n",
        "    \n",
        "    # Evaluate model accuracy/loss on test set (decrypt for evaluation)\n",
        "    target_model.decrypt()\n",
        "    test_loss, test_acc = evaluate_accuracy_loss(target_model, test_loader, criterion, device='cpu')\n",
        "    print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.2f}%\")\n",
        "    target_model.encrypt()\n",
        "    \n",
        "    # Reuse attack model from plaintext equivalent\n",
        "    arch_key = name.replace('Mpc', '')\n",
        "    attack_model = attack_models[arch_key]\n",
        "    print(f\"Reusing attack model from PlainText{arch_key}\")\n",
        "    \n",
        "    # Evaluate MIA\n",
        "    acc, prec, rec = evaluate_mia_attack(\n",
        "        target_model=target_model,\n",
        "        attack_model=attack_model,\n",
        "        train_loader=target_train_loader_mpc,\n",
        "        test_loader=target_test_loader,\n",
        "        device=device,\n",
        "        is_mpc=True\n",
        "    )\n",
        "    results[name] = {\n",
        "        'mia_accuracy': acc, 'mia_precision': prec, 'mia_recall': rec,\n",
        "        'test_loss': test_loss, 'test_accuracy': test_acc, 'is_mpc': True\n",
        "    }\n",
        "\n",
        "\n",
        "# Summary\n",
        "\n",
        "print(f\"\\n{'='*90}\")\n",
        "print(\"RESULTS SUMMARY\")\n",
        "print(f\"{'='*90}\")\n",
        "print(f\"{'Model':<25} {'Type':<10} {'Test Acc':<10} {'Test Loss':<10} {'MIA Acc':<10} {'MIA Prec':<10} {'MIA Rec':<10}\")\n",
        "print(\"-\" * 90)\n",
        "for name, res in sorted(results.items()):\n",
        "    t_type = 'MPC' if res['is_mpc'] else 'Plaintext'\n",
        "    print(f\"{name:<25} {t_type:<10} {res['test_accuracy']:<10.2f} {res['test_loss']:<10.4f} {res['mia_accuracy']:<10.2f} {res['mia_precision']:<10.2f} {res['mia_recall']:<10.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "crypten_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
